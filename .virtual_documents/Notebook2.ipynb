import os
os.environ['KAGGLE_USERNAME'] = 'your_kaggle_username'
os.environ['KAGGLE_KEY']      = 'your_kaggle_key'
import json
import pandas as pd
from kaggle.api.kaggle_api_extended import KaggleApi
from pymongo import MongoClient
import schedule
import time









# what do download and open connection
DATASET = 'zynicide/wine-reviews'      # Kaggle dataset identifier (owner/dataset)
DATA_PATH = '/Users/catarina/Desktop/notebooks/data_notebook2'                    # Local folder for raw files
MONGO_URI = 'mongodb://localhost:27017'  # MongoDB connection string
DB_NAME = 'etl_db'                    # Database name
COLLECTION_NAME = 'wine_reviews'      # Collection name in MongoDB

# Ensure data directory exists
os.makedirs(DATA_PATH, exist_ok=True)



# use credentials to actually download
KAGGLE_CONFIG_DIR = os.path.expanduser('~/.kaggle')
KAGGLE_JSON_PATH = os.path.join(KAGGLE_CONFIG_DIR, 'kaggle.json')

if not os.path.exists(KAGGLE_JSON_PATH):
    user = os.getenv('KAGGLE_USERNAME')
    key = os.getenv('KAGGLE_KEY')
    if user and key:
        os.makedirs(KAGGLE_CONFIG_DIR, exist_ok=True)
        with open(KAGGLE_JSON_PATH, 'w') as f:
            json.dump({'username': user, 'key': key}, f)
        os.chmod(KAGGLE_JSON_PATH, 0o600)
        print('Generated kaggle.json from environment variables.')
    else:
        raise OSError(
            f"Could not find kaggle.json at {KAGGLE_JSON_PATH}.\n"
            "Please place your kaggle.json there or set KAGGLE_USERNAME and KAGGLE_KEY environment variables."
        )

# function to get the csv file
def extract() -> str:
    """
    Get the file and indicates where is it
    """
    api = KaggleApi()
    api.authenticate()
    api.dataset_download_files(DATASET, path=DATA_PATH, unzip=True)
    print("Download/unzip completed")
    # Locate the first CSV file in the folder
    for fname in os.listdir(DATA_PATH):
        if fname.lower().endswith('.csv'):
            return os.path.join(DATA_PATH, fname)
    raise FileNotFoundError("No CSV file found in data directory.")

# get the file into a dataframe. check if everything if ok
# remembering the nosql format, use dictionary
def transform(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    df['review_length'] = df['description'].astype(str).str.len()

    if df[['title', 'points', 'description']].isnull().any().any():
        raise ValueError("Nulls found in one of title/points/description")

    if not df['points'].between(0, 100).all():
        invalid = df.loc[~df['points'].between(0, 100), 'points']
        raise ValueError(f"'points' outside 0-100 range: {invalid.tolist()[:10]}")

    print("Transformation and validation passed.")
    return df

# load the data into the database
def load(df: pd.DataFrame) -> None:
    client = MongoClient(MONGO_URI)
    db = client[DB_NAME]
    collection = db[COLLECTION_NAME]
    if df.empty:
        print("No rows in DataFrame to insert.")
        return

    records = df.to_dict(orient='records')
    result  = collection.insert_many(records)
    print(f"Inserted {len(result.inserted_ids)} records into MongoDB.")


# etl function to run the above functions
def run_etl():
    try:
        print('Starting ETL process...')
        csv_file = extract()
        print(f'Extracted to {csv_file}')
        data = transform(csv_file)
        print(f'Transformed {len(data)} records')
        load(data)
        print('ETL run completed.\n')
    except Exception as e:
        print(f'ETL process failed: {e}')

# if i want to automatize it to run at a given time
# schedule.every().day.at('00:00').do(run_etl)
if __name__ == '__main__':
    print('ETL scheduler started. Press Ctrl+C to exit.')
    run_etl()  # Initial run
    while True:
        schedule.run_pending()
        time.sleep(30)  # check every 30 seconds




