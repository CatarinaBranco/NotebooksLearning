#!pip install pymysql
#!pip install --upgrade pip
#!pip install psycopg2 (for PostgreSQL databases)
#!pip install kagglehub
#!pip install psycopg2-binary ipython-sql
import psycopg2 # postgreSQL driver for python. To connect.
from sqlalchemy import create_engine, text # testx wraps a raw SQL string into a TextClause object
import os
import pandas as pd
os.environ["KAGGLEHUB_CACHE"] = "/Users/catarina/Desktop/EMDYN"
import kagglehub
import subprocess # very useful module that allows me to more elegantly run bash commands
from IPython.display import HTML, display # like the sql, to read html code
import pymongo
from bson import ObjectId  # for handling ObjectId fields if needed
from datetime import datetime





def reverse_list(lst):
    """
    Return a new list that is the reverse of lst.
    """
    return lst[::-1]




# Example:
original = [1, 2, 3, 4, 5]
reversed_list = reverse_list(original)

print("Original:", original)      # Original: [1, 2, 3, 4, 5]
print("Reversed:", reversed_list) # Reversed: [5, 4, 3, 2, 1]

original = ['water', 'fire', 'land', 'air']
reversed_list = reverse_list(original)

print("Original:", original)      # Original: [1, 2, 3, 4, 5]
print("Reversed:", reversed_list) # Reversed: [5, 4, 3, 2, 1]





















# Dictonary example
dictionary = {
    'a': 1,
    'b': 9, 
    'c': 'C',
    'd': True
}

# insert
dictionary['e'] = False

# delete
del dictionary['a']

# search 
print(dictionary['c'])








# download dataset
path = kagglehub.dataset_download("hummaamqaasim/jobs-in-data")
print("Path to dataset files:", path)
print(os.listdir(path)[0])

file=path+'/'+(os.listdir(path)[0])
print(file)


# read the file into pandas
df = pd.read_csv(file)

# show first rows
print("Loaded CSV with shape:", df.shape)
df.head()


df.describe(include='all')


!brew services start postgresql
!pg_isready
%load_ext sql


# Function to help with the bash commands, using the subprocess module I called earlier
def run_shell_command(cmd):
    result = subprocess.run(cmd, capture_output=True, text=True)
    return result

# Create mydata database, or say if it already exists
db_check = run_shell_command([
    "psql", "-U", "postgres", "-tAc",
    "SELECT 1 FROM pg_database WHERE datname='mydata';"
])
if db_check.stdout.strip() == "1":
    print("Database 'mydata' already exists.")
else:
    create_db = run_shell_command(["createdb", "employees"])
    if create_db.returncode == 0:
        print("Database 'mydata' created successfully.")
    else:
        print("Error creating database 'mydata':", create_db.stderr)

# Create my username or say if already exists 
user_check = run_shell_command([
    "psql", "-U", "postgres", "-tAc",
    "SELECT 1 FROM pg_roles WHERE rolname='cbranco';"
])
if user_check.stdout.strip() == "1":
    print("User 'cbranco' already exists.")
else:
    create_user = run_shell_command([
        "psql", "-U", "postgres", "-c",
        "CREATE USER cbranco WITH PASSWORD '0000';"
    ])
    if create_user.returncode == 0:
        print("User 'cbranco' created successfully.")
    else:
        print("Error creating user 'cbranco':", create_user.stderr)

# 3) Grant privileges on 'mydata' to 'cbranco'
grant_privs = run_shell_command([
    "psql", "-U", "postgres", "-c",
    "GRANT ALL PRIVILEGES ON DATABASE mydata TO cbranco;"
])
if grant_privs.returncode == 0:
    print("Granted all privileges on 'mydata' to 'cbranco'.")
else:
    print("Error granting privileges:", grant_privs.stderr)



# Reuse your Postgres credentials
DB_USER     = "cbranco"
DB_PASSWORD = "0000"
DB_HOST     = "localhost"
DB_PORT     = "5432"
DB_NAME     = "mydata"

engine = create_engine(
    f"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
)

#send the df to sql table (employees)
df.to_sql(
    name="employees",
    con=engine,
    if_exists="replace",
    index=False
)

print("✅ CSV has been written into Postgres as table `employees`.")


# Read back the first 5 rows from Postgres
df2 = pd.read_sql_query("SELECT * FROM employees LIMIT 5;", con=engine)
df2


sql_snd_dalary = """

SELECT
  salary_in_usd
FROM (
  SELECT
    salary_in_usd,
    DENSE_RANK() OVER (ORDER BY salary_in_usd DESC) AS rnk
  FROM
    employees
) AS ranked_salaries
WHERE
  rnk = 2;
"""


snd_salary = pd.read_sql_query(sql_snd_dalary, con=engine)


print(snd_salary)


# check which country pays the most
# to run the sql query, save it to a variable

sql_highest_salary="""
SELECT
  company_location,
  ROUND(AVG(salary_in_usd)::numeric, 0) AS avg_salary,
  COUNT(*) AS num_jobs
FROM
  employees
GROUP BY
  company_location
ORDER BY
  avg_salary DESC
LIMIT 10;
"""


highest_salary = pd.read_sql_query(sql_highest_salary, con=engine)


print(highest_salary)











# create a table for the social media application users and followers. Do it in a single sql instruction. 
# with the """ I can write exactly how I would write in an SQL query.
create_users_followers_tables_sql = """
-- Create users table
CREATE TABLE IF NOT EXISTS users (
  id             BIGSERIAL     PRIMARY KEY,
  username       VARCHAR(50)   NOT NULL UNIQUE,
  email          VARCHAR(255)  NOT NULL UNIQUE,
  created_at     TIMESTAMP     NOT NULL DEFAULT NOW(),
  password_hash  VARCHAR(255)  NOT NULL,
  CHECK (username <> '')
);

-- Create follows table
CREATE TABLE IF NOT EXISTS follows (
  follower_id BIGINT    NOT NULL,
  followee_id BIGINT    NOT NULL,
  created_at  TIMESTAMP NOT NULL DEFAULT NOW(),

  PRIMARY KEY (follower_id, followee_id), -- single combinations, prevent repetitions

  FOREIGN KEY (follower_id)
    REFERENCES users (id)
    ON DELETE CASCADE,
  FOREIGN KEY (followee_id) -- if a user is deleted the whole cascade is also deleted from the table
    REFERENCES users (id)
    ON DELETE CASCADE,

  CHECK (follower_id <> followee_id)
);

-- Indexes to speed up lookups
CREATE INDEX IF NOT EXISTS idx_follows_follower ON follows (follower_id);
CREATE INDEX IF NOT EXISTS idx_follows_followee ON follows (followee_id);
"""

# Execute the DDL in PostgreSQL
with engine.begin() as conn:
    conn.execute(text(create_users_followers_tables_sql))

print("✅ Tables `users` and `follows` have been created (or already existed).")




# create some mockup data
insert_users_sql = """
INSERT INTO users (username, email, password_hash)
VALUES
  ('alice',   'alice@example.com',   '$2b$12$abcdefghijk01234567890123456789012345678901234567890'),  -- pretend bcrypt hash
  ('bob',     'bob@example.com',     '$2b$12$mnopqrstuv34567890123456789012345678901234567890123456'),
  ('carol',   'carol@example.com',   '$argon2i$v=19$m=65536,t=2,p=1$abcd1234$xyz9876543210abcdef'),
  ('dave',    'dave@example.com',    '$argon2i$v=19$m=65536,t=2,p=1$wxyz5678$lmn543210abcdef987654'),
  ('erin',    'erin@example.com',    '$2b$12$uvwxyz98765432101234567890123456789012345678901234567')
ON CONFLICT (username) DO NOTHING;
"""  

with engine.begin() as conn:
    conn.execute(text(insert_users_sql))

print("✅ Sample rows inserted into `users`.")


# get the users id, so that then I can fake follow events. 
with engine.connect() as conn:
    users_df = pd.read_sql("SELECT id, username FROM users ORDER BY id;", conn)

print("\nCurrent users (id ↔ username):")
print(users_df.to_string(index=False))

# build a dictionary where the username is teh dictionary keym and id is the value: username → id
user_id_map = dict(zip(users_df["username"], users_df["id"]))

# follow‐rows based on those IDs
follows_rows = [
    (user_id_map["alice"], user_id_map["bob"]),
    (user_id_map["alice"], user_id_map["carol"]),
    (user_id_map["bob"],   user_id_map["alice"]),
    (user_id_map["carol"], user_id_map["dave"]),
    (user_id_map["carol"], user_id_map["erin"]),
    (user_id_map["erin"],  user_id_map["alice"]),
]

# insert them with “ON CONFLICT DO NOTHING” to avoid duplicates
insert_follows = text("""
INSERT INTO follows (follower_id, followee_id)
VALUES (:follower_id, :followee_id)
ON CONFLICT (follower_id, followee_id) DO NOTHING;
""")

with engine.begin() as conn:
    for follower_id, followee_id in follows_rows:
        conn.execute(insert_follows, {"follower_id": follower_id, "followee_id": followee_id})

print("\n✅ Sample rows inserted into `follows`.")



# show
with engine.connect() as conn:
    users_full = pd.read_sql("SELECT * FROM users ORDER BY id;", conn)
    follows_full = pd.read_sql("""
        SELECT
            f.follower_id,
            u1.username AS follower_username,
            f.followee_id,
            u2.username AS followee_username,
            f.created_at
        FROM follows AS f
        JOIN users u1 ON u1.id = f.follower_id
        JOIN users u2 ON u2.id = f.followee_id
        ORDER BY f.follower_id, f.followee_id;
    """, conn)

print("\n--- `users` table: ---")
print(users_full.to_string(index=False))

print("\n--- `follows` table (joined with usernames): ---")
print(follows_full.to_string(index=False))



# who is'alice' following
alice_id = user_id_map['alice']
following_df = pd.read_sql(text("""
    SELECT 
        u.id AS user_id, 
        u.username AS username, 
        f.created_at AS followed_at
    FROM follows f
    JOIN users u ON u.id = f.followee_id
    WHERE f.follower_id = :alice_id
    ORDER BY u.username;
"""), engine, params={"alice_id": alice_id})

print("Users Alice is following:")
print(following_df.to_string(index=False))











# To delete documents of the collection
result = collection.delete_many({})


# connect 
MONGO_URI = "mongodb://localhost:27017"  
client = pymongo.MongoClient(MONGO_URI)

# create a database and a collection
db = client["my_database"]            # creates my_database
collection = db["users_collection"]   # creates users_collection

# insert a few sample documents 
sample_docs = [
    {
        "username": "alice",
        "email": "alice@example.com",
        "profile": {
            "first_name": "Alice", # no age
            "last_name": "Johnson"
            # no age in this profile
        },
        "adress": "USA", # only document with this field
        "roles": ["admin", "editor"],
        "created_at": pd.Timestamp("2025-06-05 09:00:00")  
    },
    {
        "username": "bob",
        "email": "bob@example.com",
        "profile": {
            "first_name": "Bob",
            "last_name": "Smith",
            "age": 28
        },
        "roles": ["editor"],
        "created_at": pd.Timestamp("2025-06-04 14:30:00")
    },
    {
        "username": "carol",
        "email": "carol@example.com",
        "profile": {
            "first_name": "Carol",
            "last_name": "Williams",
            "age": 25
        },
        "roles": ["viewer"],
        "created_at": pd.Timestamp("2025-06-02 16:15:00")
    }
]

# insert if collection is empty (so re-running doesn’t duplicate)
if collection.count_documents({}) == 0:
    collection.insert_many(sample_docs)
    print("Inserted sample documents into 'users_collection'.")







# find() on a collection searches from a document where the field role has 'editor'
results_cursor = collection.find({"roles": "editor"})
results_list = list(results_cursor) # a list whose elemnsta are dictionaries
# print(results_list)
for doc in results_list:
    # Convert ObjectId to string for nicer display
    doc["_id"] = str(doc["_id"])
# print(results_list) # Improved formatting
users_df = pd.DataFrame(results_list)

print("Documents where roles include 'editor':")
display(users_df)
print(users_df)


# Who is older than 26?
cursor = collection.find({"profile.age": {"$gt": 26}}).sort("profile.age", pymongo.ASCENDING)
df_over_26 = pd.DataFrame(list(cursor))
df_over_26["_id"] = df_over_26["_id"].astype(str)  # stringify the ObjectId
display(df_over_26)


# Add a new field to only one document
collection.update_one(
    {"username": "bob"},
    {"$set": {"last_login": datetime.utcnow()}}
)


# Delete one document based on username
collection.delete_one({"username": "carol"})
print("Deleted Carol’s document, if it existed.")


# Show all documents
docs = list(collection.find({}))
for d in docs:
    d["_id"] = str(d["_id"])
df = pd.DataFrame(docs)
display(df)



