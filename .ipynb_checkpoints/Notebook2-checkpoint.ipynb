{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240cd59d-1af9-41d9-930b-4439fba68dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = 'your_kaggle_username'\n",
    "os.environ['KAGGLE_KEY']      = 'your_kaggle_key'\n",
    "import json\n",
    "import pandas as pd\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from pymongo import MongoClient\n",
    "import schedule\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b89748-9761-43b9-9d9d-46815bad9cfa",
   "metadata": {},
   "source": [
    "# Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d2476-ab46-47d0-987f-cf90cbe57c26",
   "metadata": {},
   "source": [
    "## 2.1 Design an ETL pipeline that ingests JSON data from an API into a data warehouse every hour. Outline the major steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e9aeae-8e4d-427c-b021-dd3e30900080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do download and open connection\n",
    "DATASET = 'zynicide/wine-reviews'      # Kaggle dataset identifier (owner/dataset)\n",
    "DATA_PATH = '/Users/catarina/Desktop/notebooks/data_notebook2'                    # Local folder for raw files\n",
    "MONGO_URI = 'mongodb://localhost:27017'  # MongoDB connection string\n",
    "DB_NAME = 'etl_db'                    # Database name\n",
    "COLLECTION_NAME = 'wine_reviews'      # Collection name in MongoDB\n",
    "\n",
    "# Ensure data directory exists\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26cc088-8a47-4f25-b20b-678eef65acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use credentials to actually download\n",
    "KAGGLE_CONFIG_DIR = os.path.expanduser('~/.kaggle')\n",
    "KAGGLE_JSON_PATH = os.path.join(KAGGLE_CONFIG_DIR, 'kaggle.json')\n",
    "\n",
    "if not os.path.exists(KAGGLE_JSON_PATH):\n",
    "    user = os.getenv('KAGGLE_USERNAME')\n",
    "    key = os.getenv('KAGGLE_KEY')\n",
    "    if user and key:\n",
    "        os.makedirs(KAGGLE_CONFIG_DIR, exist_ok=True)\n",
    "        with open(KAGGLE_JSON_PATH, 'w') as f:\n",
    "            json.dump({'username': user, 'key': key}, f)\n",
    "        os.chmod(KAGGLE_JSON_PATH, 0o600)\n",
    "        print('Generated kaggle.json from environment variables.')\n",
    "    else:\n",
    "        raise OSError(\n",
    "            f\"Could not find kaggle.json at {KAGGLE_JSON_PATH}.\\n\"\n",
    "            \"Please place your kaggle.json there or set KAGGLE_USERNAME and KAGGLE_KEY environment variables.\"\n",
    "        )\n",
    "\n",
    "# function to get the csv file\n",
    "def extract() -> str:\n",
    "    \"\"\"\n",
    "    Get the file and indicates where is it\n",
    "    \"\"\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_download_files(DATASET, path=DATA_PATH, unzip=True)\n",
    "    print(\"Download/unzip completed\")\n",
    "    # Locate the first CSV file in the folder\n",
    "    for fname in os.listdir(DATA_PATH):\n",
    "        if fname.lower().endswith('.csv'):\n",
    "            return os.path.join(DATA_PATH, fname)\n",
    "    raise FileNotFoundError(\"No CSV file found in data directory.\")\n",
    "\n",
    "# get the file into a dataframe. check if everything if ok\n",
    "# remembering the nosql format, use dictionary\n",
    "def transform(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['review_length'] = df['description'].astype(str).str.len()\n",
    "\n",
    "    if df[['title', 'points', 'description']].isnull().any().any():\n",
    "        raise ValueError(\"Nulls found in one of title/points/description\")\n",
    "\n",
    "    if not df['points'].between(0, 100).all():\n",
    "        invalid = df.loc[~df['points'].between(0, 100), 'points']\n",
    "        raise ValueError(f\"'points' outside 0-100 range: {invalid.tolist()[:10]}\")\n",
    "\n",
    "    print(\"Transformation and validation passed.\")\n",
    "    return df\n",
    "\n",
    "# load the data into the database\n",
    "def load(df: pd.DataFrame) -> None:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[DB_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "    if df.empty:\n",
    "        print(\"No rows in DataFrame to insert.\")\n",
    "        return\n",
    "\n",
    "    records = df.to_dict(orient='records')\n",
    "    result  = collection.insert_many(records)\n",
    "    print(f\"Inserted {len(result.inserted_ids)} records into MongoDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a7ddf1-ac70-469d-be24-4454bc10b5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL scheduler started. Press Ctrl+C to exit.\n",
      "Starting ETL process...\n",
      "Dataset URL: https://www.kaggle.com/datasets/zynicide/wine-reviews\n",
      "Download/unzip completed\n",
      "Extracted to /Users/catarina/Desktop/notebooks/data_notebook2/winemag-data-130k-v2.csv\n",
      "Transformation and validation passed.\n",
      "Transformed 129971 records\n",
      "Inserted 129971 records into MongoDB.\n",
      "ETL run completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# etl function to run the above functions\n",
    "def run_etl():\n",
    "    try:\n",
    "        print('Starting ETL process...')\n",
    "        csv_file = extract()\n",
    "        print(f'Extracted to {csv_file}')\n",
    "        data = transform(csv_file)\n",
    "        print(f'Transformed {len(data)} records')\n",
    "        load(data)\n",
    "        print('ETL run completed.\\n')\n",
    "    except Exception as e:\n",
    "        print(f'ETL process failed: {e}')\n",
    "\n",
    "# if i want to automatize it to run at a given time\n",
    "# schedule.every().day.at('00:00').do(run_etl)\n",
    "if __name__ == '__main__':\n",
    "    print('ETL scheduler started. Press Ctrl+C to exit.')\n",
    "    run_etl()  # Initial run\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(30)  # check every 30 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a761d7-c2a6-44d1-b5ce-88cd0eac1b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
